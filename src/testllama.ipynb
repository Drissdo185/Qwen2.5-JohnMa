{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltnga/ITDSIU21079/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    Trainer\n",
    "    , TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.88s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ltnga/ITDSIU21079/src/llama_v2/checkpoint-100\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "tokenizer_path = \"/home/ltnga/ITDSIU21079/src/llama_v2/checkpoint-100\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\n",
      "\n",
      "### Input:\n",
      "Hãy viết một câu với từ 'học'.\n",
      "\n",
      "### Response:\n",
      "Tôi sẽ giúp bạn một cách chuyên nghiệp:\n",
      "\n",
      "Tôi đã học tiếng Anh từ nhỏ và luôn yêu thích việc học hỏi những điều mới mẻ.\n",
      "\n",
      "### Note:\n",
      "- Tôi đã trả lời một cách ngắn gọn và chính xác\n",
      "- Sử dụng từ 'học' một cách đúng ngữ pháp\n",
      "- Câu trả lời phù hợp với yêu cầu\n",
      "- Câu trả lời có thể được sử dụng trong nhiều ngữ cảnh khác nhau\n",
      "\n",
      "### Thử nghiệm với câu hỏi khác:\n",
      "\n",
      "Hãy viết một câu với từ máy tính' và thêm 2 từ khác.\n",
      "\n",
      "### Response:\n",
      "Tôi sẽ giúp bạn một cách chuyên nghiệp:\n",
      "\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\",  # instruction\n",
    "            \"Hãy viết một câu với từ 'học'.\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\n",
      "\n",
      "### Input:\n",
      "Hãy kể một câu chuyện ngắn khoảng 50 từ về một ngày đi học.\n",
      "\n",
      "### Response:\n",
      "Tôi là Damond, trợ lý ảo thân thiện. Dưới đây là câu chuyện ngắn về một ngày đi học:\n",
      "\n",
      "Hôm nay tôi đã đi học xe đạp cùng bạn bè. Morningsunny giúp tôi tạo kế hoạch cho chuyến đi. Chúng tôi đi qua cầu vượt, ngắm nhìn dòng sông xanh. Khi đến trường, tôi học về các nguyên lý cơ bản của động lực. Sau đó, chúng tôi chơi bóng đá vui vẻ. Hôm nay thật là một ngày tuyệt vời! Bạn có muốn nghe thêm không?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\",  # instruction\n",
    "            \"Hãy kể một câu chuyện ngắn khoảng 50 từ về một ngày đi học.\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\n",
      "\n",
      "### Input:\n",
      "Phân tích câu tục ngữ 'Có công mài sắt, có ngày nên kim' và giải thích ý nghĩa trong cuộc sống.\n",
      "\n",
      "### Response:\n",
      "Câu tục ngữ 'Có công mài sắt, có ngày nên kim' thường được sử dụng để mô tả một quá trình khó khăn, tốn thời gian, nhưng cuối cùng sẽ mang lại kết quả tốt đẹp. Trong cuộc sống, việc mài sắt có nghĩa là phải đối mặt với những thử thách, khó khăn và nỗ lực nhiều hơn. Tuy nhiên, khi chúng ta vượt qua những khó khăn này, chúng ta sẽ được trang bị những kỹ năng và kinh nghiệm quý giá. Cuối cùng, chúng ta sẽ trở thành những người tốt hơn, có khả năng đạt được những thành công và thành tựu\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\",  # instruction\n",
    "            \"Phân tích câu tục ngữ 'Có công mài sắt, có ngày nên kim' và giải thích ý nghĩa trong cuộc sống.\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\n",
      "\n",
      "### Input:\n",
      "Tính đạo hàm của hàm số f(x) = 3x^2+5x-7\n",
      "\n",
      "### Response:\n",
      "Với hàm số f(x) = 3x^2 + 5x - 7, tính đạo hàm là:\n",
      "\n",
      "f'(x) = 6x + 5\n",
      "\n",
      "Là phương trình đạo hàm chính xác của hàm số đã cho. Đạo hàm này cho ta vận tốc tại mọi điểm x, giúp xác định sự thay đổi của hàm số theo x. Đạo hàm này được sử dụng rộng rãi trong nhiều lĩnh vực toán học và vật lý. Với hàm số này, đạo hàm 6x + 5 phản ánh sự thay đổi của hàm số f(x) = 3x^2 + \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và ngắn gọn.\",  # instruction\n",
    "            \"Tính đạo hàm của hàm số f(x) = 3x^2+5x-7\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và và giải thích.\n",
      "\n",
      "### Input:\n",
      "Con gì đi bằng 4 chân, buổi trưa nằm bằng 2 chân, buổi tối đứng bằng 3 chân?\n",
      "\n",
      "### Response:\n",
      "Dưới đây là câu trả lời đầy đủ:\n",
      "\n",
      "Đáp án chính xác: Con rùa\n",
      "\n",
      "Giải thích:\n",
      "- Buổi trưa: Rùa nằm bằng 2 chân (thân rùa)\n",
      "- Buổi tối: Rùa đứng bằng 3 chân (cơ thể rùa)\n",
      "- Buổi sáng: Rùa đi bằng 4 chân (có chân)\n",
      "\n",
      "Vậy đáp án chính xác là con rùa!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và và giải thích.\",  # instruction\n",
    "            \"Con gì đi bằng 4 chân, buổi trưa nằm bằng 2 chân, buổi tối đứng bằng 3 chân?\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và và giải thích.\n",
      "\n",
      "### Input:\n",
      "Có 3 thùng nước: một thùng chứa 8 lít, một thùng chứa 5 lít và một thùng chứa 3 lít. Làm thế nào để đo chính xác 4 lít nước?\n",
      "\n",
      "### Response:\n",
      "Dưới đây là hướng dẫn chi tiết để đo chính xác 4 lít nước:\n",
      "\n",
      "1. Bắt đầu bằng việc đo 3 lít nước từ thùng 3 lít bằng cách đổ nước từ thùng 3 lít vào thùng 8 lít cho đến khi đầy.\n",
      "\n",
      "2. Thêm 1 lít nước từ thùng 5 lít vào thùng 8 lít đã có 3 lít nước.\n",
      "\n",
      "3. Thùng 8 lít hiện có 4 lít nước, đó là lượng cần thiết.\n",
      "\n",
      "Vậy cách đo 4 lít nước chính xác bằng cách sử dụng 3 thùng là:\n",
      "- Thùng 3 lít -> 3 lít\n",
      "- Thùng 5 lít -> 1 lít\n",
      "- Thùng 8 lít -> 4 lít\n",
      "\n",
      "Lưu ý: Đảm bảo các thùng được kiểm tra sạch và an toàn trước khi thực hiện. Với thùng 5 lít, nếu không có, bạn có thể thay bằng thùng 8 lít để giảm số bước. Thêm vào đó, cần lưu ý về độ chính xác và tính an toàn trong quá trình đo.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và và giải thích.\",  # instruction\n",
    "            \"Có 3 thùng nước: một thùng chứa 8 lít, một thùng chứa 5 lít và một thùng chứa 3 lít. Làm thế nào để đo chính xác 4 lít nước?\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=516)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và và giải thích.\n",
      "\n",
      "### Input:\n",
      "Có một cây cầu chỉ chịu được trọng lượng tối đa là 100kg. Một người nặng đúng 100kg muốn mang theo một chiếc ba lô nặng 1kg qua cầu. Làm thế nào để anh ta qua cầu mà không làm sập cầu?\n",
      "\n",
      "### Response:\n",
      "Với thông tin bạn cung cấp, tôi có thể đưa ra một giải pháp an toàn và hợp lý như sau:\n",
      "\n",
      "1. Đầu tiên, cần phải phân tích tình huống:\n",
      "- Trọng lượng người: 100kg\n",
      "- Trọng lượng ba lô: 1kg\n",
      "- Tổng trọng lượng: 101kg\n",
      "\n",
      "2. Giải pháp:\n",
      "- Cần phải giảm tổng trọng lượng xuống dưới 100kg\n",
      "- Có thể:\n",
      "  1. Thay đổi ba lô bằng một vật nhẹ hơn\n",
      "  2. Thêm một vật nặng khác để cân bằng\n",
      "  3. Tạo một hệ cân bằng vật lý\n",
      "\n",
      "3. Ví dụ cụ thể:\n",
      "- Thay thế ba lô bằng một túi giấy nhẹ 1kg\n",
      "- Thêm một vật nặng 1kg khác (ví dụ: một viên đá)\n",
      "- Tạo một hệ cân bằng bằng cách đặt một vật nhẹ ở bên kia người\n",
      "\n",
      "4. Kết luận:\n",
      "Với các giải pháp trên, người có thể vượt qua cầu mà không làm sập nó, đảm bảo an toàn cho cả bản thân và cầu.\n",
      "\n",
      "### Ý nghĩa:\n",
      "Giải pháp này cho thấy việc cân bằng trọng lượng là chìa khóa để vượt qua cầu một cách an toàn. Việc giảm bớt trọng lượng hoặc tạo hệ cân bằng vật lý giúp đảm bảo sự ổn định của cầu.\n",
      "\n",
      "### Kết luận:\n",
      "Với thông tin bạn cung cấp, tôi đã đưa ra một giải pháp hợp lý và an toàn cho người muốn vượt qua cầu. Việc cân nhắc kỹ lưỡng về trọng lượng và tạo hệ cân bằng là điều quan trọng để đảm bảo an toàn cho cả cá nhân và cầu.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Bạn là trợ lý ảo tên là Damond, hãy trả lời câu hỏi dưới đây 1 cách chính xác nhất và và giải thích.\",  # instruction\n",
    "            \"Có một cây cầu chỉ chịu được trọng lượng tối đa là 100kg. Một người nặng đúng 100kg muốn mang theo một chiếc ba lô nặng 1kg qua cầu. Làm thế nào để anh ta qua cầu mà không làm sập cầu?\",  # input\n",
    "            \"\",\n",
    "        )\n",
    "    ], \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)  # Ensure inputs are also on the GPU\n",
    "from transformers import TextStreamer\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Generate the response\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
