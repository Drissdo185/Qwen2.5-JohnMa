{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  6.02it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.87it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ltnga/ITDSIU21079/src/qwen_v2/checkpoint-100\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "tokenizer_path = \"/home/ltnga/ITDSIU21079/src/qwen_v2/checkpoint-100\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,   \n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected', 'conversations'],\n",
      "    num_rows: 43904\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 43904/43904 [00:01<00:00, 23958.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x.get(\"chosen\") and x.get(\"rejected\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'gpt', 'value': 'Người ngoại trái gió Zephyrian có khả năng nhận biết và diễn giải mẫu màu sắc và âm thanh một cách sâu sắc và liên quan chặt chẽ đến cấu trúc sinh học và xử lý thần kinh của họ. Từ góc độ sinh lý học, những sinh vật này đã tiến hóa để nhận biết một phổ màu sắc và sóng âm rộng hơn so với con người. Mắt của họ có thể phát hiện tần số ánh sáng mà con người không thể, dẫn đến khả năng nhìn thấy và diễn giải loạt màu sắc mà họ sử dụng để giao tiếp.\\n\\nHệ thống thính giác của họ, ngược lại, được điều chỉnh để phát hiện một phạm vi tần số rộng hơn, cho phép họ nhận biết và tạo ra những giai điệu mà chúng ta có thể thấy là kim loại hoặc kêu rát nhưng lại là phương tiện mạnh mẽ cho việc giao tiếp trong xã hội của họ. Ngoài ra, họ sở hữu một mạng thần kinh tiên tiến được thiết kế để giải mã những kết hợp phức tạp của màu sắc và âm thanh này, dịch chúng thành một hiểu biết toàn diện về cảm xúc, ý tưởng và ý định.\\n\\nVề việc diễn giải những mẫu màu sắc và âm thanh này, người ngoại trái gió sử dụng một bộ sưu tập phức tạp các đường dẫn thần kinh cho phép họ xử lý dữ liệu nhận được từ giác quan của họ. Khác với con người phải dựa vào ngôn ngữ nói để truyền đạt các khái niệm trừu tượng và cảm xúc, những sinh vật này xử lý thông tin một cách toàn diện, với mẫu màu sắc và âm thanh cung cấp một bức tranh thông tin phong phú.\\n\\nVí dụ, một gam màu xanh duy nhất có thể truyền đạt cảm giác bình yên hoặc biểu thị một người thân yêu, trong khi một nốt nhạc nhanh, sắc bén có thể biểu thị sự hứng thú hoặc nguy hiểm. Sự kết hợp của những yếu tố này cho phép họ thể hiện và diễn giải một loạt cảm xúc và khái niệm trí tuệ của con người, giống như một nhạc sĩ tạo ra một bản giao hưởng hoặc một họa sĩ tạo ra một kiệt tác.\\n\\nHiểu và diễn giải những mẫu màu sắc này là bẩm sinh đối với họ. Họ được sinh ra với khả năng này, giống như cách con người học cách hiểu ngôn ngữ từ khi còn nhỏ. Mặc dù quá trình nhận thức và thần kinh của họ có thể khác với chúng ta, họ trải nghiệm và xử lý những mẫu đa giác quan này một cách tự nhiên và trực giác. Khả năng này là một phần không thể thiếu của bản sắc của họ, định hình văn hóa, động lực xã hội và tương tác hàng ngày của họ.'}\n",
      "{'from': 'gpt', 'value': \"Người ngoại trái gió Zephyrian sở hữu một hệ thống giác quan phát triển cao cho phép họ nhận biết và diễn giải các mẫu màu sắc và âm thanh được tạo ra bởi loài của chính mình.\\n\\nĐối với các mẫu màu sắc, mắt của sinh vật này phát triển hơn rất nhiều so với mắt của con người. Họ có thể nhận biết một phổ màu rộng hơn, bao gồm cả tia tử ngoại và hồng ngoại, và có khả năng phát hiện những thay đổi nhỏ về màu sắc và mẫu vẽ. Điều này cho phép họ diễn giải các hiển thị màu sắc phức tạp được tạo ra bởi vảy của họ.\\n\\nHơn nữa, não bộ của họ được kết nối để xử lý các tín hiệu hình ảnh này một cách hoàn toàn khác biệt so với con người. Nơi chúng ta nhìn thấy màu sắc như một thuộc tính tĩnh, họ cảm nhận nó như một khía cạnh động, luôn thay đổi của môi trường xung quanh họ. Họ có khả năng phân biệt những thay đổi màu sắc và mẫu vẽ tinh tế mang đến một lượng thông tin phong phú, từ trạng thái cảm xúc của cá nhân đến sự phức tạp của các khái niệm trừu tượng.\\n\\nVề mẫu âm thanh, hệ thống thính giác của sinh vật này cũng phát triển tương đương. Họ có thể nghe được một phạm vi tần số rộng hơn so với con người, và não bộ của họ có khả năng phân tích những giai điệu phức tạp được tạo ra bởi loài của họ. Họ có thể phân biệt được âm cao, nhịp điệu và hòa âm của những giai điệu này, diễn giải chúng như các 'từ' hoặc 'cụm từ' riêng biệt trong ngôn ngữ của họ.\\n\\nNgoài ra, các giác quan thính giác và hình ảnh của sinh vật này được kết nối chặt chẽ, cho phép họ xử lý các mẫu màu sắc và âm thanh đồng thời. Sự tích hợp giác quan này giúp sinh vật hiểu rõ sâu sắc và phức tạp của giao tiếp của loài của mình, vì mỗi giai điệu tăng cường ý nghĩa của mẫu màu sắc, và ngược lại.\\n\\nThú vị thay, hình thức giao tiếp này cũng có một yếu tố cảm xúc mạnh mẽ. Sinh vật không chỉ hiểu ý nghĩa đen của các mẫu màu sắc và âm thanh; họ cũng trải qua một phản ứng cảm xúc đối với chúng. Một giai điệu êm dịu kèm theo màu sắc mát mẻ có thể gợi lên cảm giác bình yên, trong khi một giai điệu nhanh, cao với màu sắc sáng chói có thể gây ra sự hồi hộp hoặc lo lắng.\\n\\nTheo cách này, cách nhìn của người ngoại trái gió Zephyrian về giao tiếp của loài của chính mình không chỉ là một quá trình nhận thức, mà còn là một trải nghiệm cảm xúc và giác quan bao gồm một loạt các tác nhân và phản ứng.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"chosen\"])\n",
    "print(dataset[0][\"rejected\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt(example):\n",
    "    chosen = example.get(\"chosen\", \"\")\n",
    "    rejected = example.get(\"rejected\", \"\")\n",
    "    \n",
    "    if isinstance(chosen, str) and isinstance(rejected, str):\n",
    "        for idx in range(min(len(chosen), len(rejected))):\n",
    "            if chosen[idx] != rejected[idx]:\n",
    "                if chosen[idx - 1] == \" \":  # Remove space before the prompt\n",
    "                    idx -= 1\n",
    "                return {\"chosen\": chosen, \"rejected\": rejected[:idx]}\n",
    "    \n",
    "    # If data is invalid, return the example unchanged\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompts from dataset: 100%|██████████| 43904/43904 [00:07<00:00, 5736.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset.map(\n",
    "    extract_prompt,\n",
    "    num_proc=training_args.dataset_num_proc,\n",
    "    desc=\"Extracting prompts from dataset\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt from train dataset:   0%|          | 0/43904 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m DPOConfig(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2.5-DPO\u001b[39m\u001b[38;5;124m\"\u001b[39m, logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDPOTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:443\u001b[0m, in \u001b[0;36mDPOTrainer.__init__\u001b[0;34m(self, model, ref_model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Compute that only on the main process for faster data processing.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# see: https://github.com/huggingface/trl/pull/1255\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mlocal_main_process_first():\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Extract the prompt if needed, and apply the chat template if needed\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaybe_extract_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_num_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExtracting prompt from train dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    447\u001b[0m         maybe_apply_chat_template,\n\u001b[1;32m    448\u001b[0m         fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: processing_class},\n\u001b[1;32m    449\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset_num_proc,\n\u001b[1;32m    450\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying chat template to train dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3446\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3444\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3446\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3448\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3338\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3337\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3338\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3340\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3341\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3342\u001b[0m     }\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/trl/data_utils.py:414\u001b[0m, in \u001b[0;36mmaybe_extract_prompt\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (chosen_conv \u001b[38;5;129;01mand\u001b[39;00m prompt_conv) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m chosen_conv \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prompt_conv):\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m example\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchosen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchosen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrejected\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrejected\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ITDSIU21079/venv/lib/python3.10/site-packages/trl/data_utils.py:314\u001b[0m, in \u001b[0;36mextract_prompt\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mExtracts the shared prompt from a preference data example, where the prompt is implicit within both\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mthe chosen and rejected completions.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mFor more details, see [`maybe_extract_prompt`].\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m\"\u001b[39m]))):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchosen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx]:\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# remove space before the prompt\u001b[39;00m\n\u001b[1;32m    316\u001b[0m             idx \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "training_args = DPOConfig(output_dir=\"Qwen2.5-DPO\", logging_steps=10)\n",
    "trainer = DPOTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
