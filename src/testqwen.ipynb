{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltnga/ITDSIU21079/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer\n",
    "    ,TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import os \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading adapter weights from DrissDo/Qwen2.5-3B-JohnMa led to missing keys in the model: model.layers.0.self_attn.q_proj.lora_A.default.weight, model.layers.0.self_attn.q_proj.lora_B.default.weight, model.layers.0.self_attn.v_proj.lora_A.default.weight, model.layers.0.self_attn.v_proj.lora_B.default.weight, model.layers.1.self_attn.q_proj.lora_A.default.weight, model.layers.1.self_attn.q_proj.lora_B.default.weight, model.layers.1.self_attn.v_proj.lora_A.default.weight, model.layers.1.self_attn.v_proj.lora_B.default.weight, model.layers.2.self_attn.q_proj.lora_A.default.weight, model.layers.2.self_attn.q_proj.lora_B.default.weight, model.layers.2.self_attn.v_proj.lora_A.default.weight, model.layers.2.self_attn.v_proj.lora_B.default.weight, model.layers.3.self_attn.q_proj.lora_A.default.weight, model.layers.3.self_attn.q_proj.lora_B.default.weight, model.layers.3.self_attn.v_proj.lora_A.default.weight, model.layers.3.self_attn.v_proj.lora_B.default.weight, model.layers.4.self_attn.q_proj.lora_A.default.weight, model.layers.4.self_attn.q_proj.lora_B.default.weight, model.layers.4.self_attn.v_proj.lora_A.default.weight, model.layers.4.self_attn.v_proj.lora_B.default.weight, model.layers.5.self_attn.q_proj.lora_A.default.weight, model.layers.5.self_attn.q_proj.lora_B.default.weight, model.layers.5.self_attn.v_proj.lora_A.default.weight, model.layers.5.self_attn.v_proj.lora_B.default.weight, model.layers.6.self_attn.q_proj.lora_A.default.weight, model.layers.6.self_attn.q_proj.lora_B.default.weight, model.layers.6.self_attn.v_proj.lora_A.default.weight, model.layers.6.self_attn.v_proj.lora_B.default.weight, model.layers.7.self_attn.q_proj.lora_A.default.weight, model.layers.7.self_attn.q_proj.lora_B.default.weight, model.layers.7.self_attn.v_proj.lora_A.default.weight, model.layers.7.self_attn.v_proj.lora_B.default.weight, model.layers.8.self_attn.q_proj.lora_A.default.weight, model.layers.8.self_attn.q_proj.lora_B.default.weight, model.layers.8.self_attn.v_proj.lora_A.default.weight, model.layers.8.self_attn.v_proj.lora_B.default.weight, model.layers.9.self_attn.q_proj.lora_A.default.weight, model.layers.9.self_attn.q_proj.lora_B.default.weight, model.layers.9.self_attn.v_proj.lora_A.default.weight, model.layers.9.self_attn.v_proj.lora_B.default.weight, model.layers.10.self_attn.q_proj.lora_A.default.weight, model.layers.10.self_attn.q_proj.lora_B.default.weight, model.layers.10.self_attn.v_proj.lora_A.default.weight, model.layers.10.self_attn.v_proj.lora_B.default.weight, model.layers.11.self_attn.q_proj.lora_A.default.weight, model.layers.11.self_attn.q_proj.lora_B.default.weight, model.layers.11.self_attn.v_proj.lora_A.default.weight, model.layers.11.self_attn.v_proj.lora_B.default.weight, model.layers.12.self_attn.q_proj.lora_A.default.weight, model.layers.12.self_attn.q_proj.lora_B.default.weight, model.layers.12.self_attn.v_proj.lora_A.default.weight, model.layers.12.self_attn.v_proj.lora_B.default.weight, model.layers.13.self_attn.q_proj.lora_A.default.weight, model.layers.13.self_attn.q_proj.lora_B.default.weight, model.layers.13.self_attn.v_proj.lora_A.default.weight, model.layers.13.self_attn.v_proj.lora_B.default.weight, model.layers.14.self_attn.q_proj.lora_A.default.weight, model.layers.14.self_attn.q_proj.lora_B.default.weight, model.layers.14.self_attn.v_proj.lora_A.default.weight, model.layers.14.self_attn.v_proj.lora_B.default.weight, model.layers.15.self_attn.q_proj.lora_A.default.weight, model.layers.15.self_attn.q_proj.lora_B.default.weight, model.layers.15.self_attn.v_proj.lora_A.default.weight, model.layers.15.self_attn.v_proj.lora_B.default.weight, model.layers.16.self_attn.q_proj.lora_A.default.weight, model.layers.16.self_attn.q_proj.lora_B.default.weight, model.layers.16.self_attn.v_proj.lora_A.default.weight, model.layers.16.self_attn.v_proj.lora_B.default.weight, model.layers.17.self_attn.q_proj.lora_A.default.weight, model.layers.17.self_attn.q_proj.lora_B.default.weight, model.layers.17.self_attn.v_proj.lora_A.default.weight, model.layers.17.self_attn.v_proj.lora_B.default.weight, model.layers.18.self_attn.q_proj.lora_A.default.weight, model.layers.18.self_attn.q_proj.lora_B.default.weight, model.layers.18.self_attn.v_proj.lora_A.default.weight, model.layers.18.self_attn.v_proj.lora_B.default.weight, model.layers.19.self_attn.q_proj.lora_A.default.weight, model.layers.19.self_attn.q_proj.lora_B.default.weight, model.layers.19.self_attn.v_proj.lora_A.default.weight, model.layers.19.self_attn.v_proj.lora_B.default.weight, model.layers.20.self_attn.q_proj.lora_A.default.weight, model.layers.20.self_attn.q_proj.lora_B.default.weight, model.layers.20.self_attn.v_proj.lora_A.default.weight, model.layers.20.self_attn.v_proj.lora_B.default.weight, model.layers.21.self_attn.q_proj.lora_A.default.weight, model.layers.21.self_attn.q_proj.lora_B.default.weight, model.layers.21.self_attn.v_proj.lora_A.default.weight, model.layers.21.self_attn.v_proj.lora_B.default.weight, model.layers.22.self_attn.q_proj.lora_A.default.weight, model.layers.22.self_attn.q_proj.lora_B.default.weight, model.layers.22.self_attn.v_proj.lora_A.default.weight, model.layers.22.self_attn.v_proj.lora_B.default.weight, model.layers.23.self_attn.q_proj.lora_A.default.weight, model.layers.23.self_attn.q_proj.lora_B.default.weight, model.layers.23.self_attn.v_proj.lora_A.default.weight, model.layers.23.self_attn.v_proj.lora_B.default.weight, model.layers.24.self_attn.q_proj.lora_A.default.weight, model.layers.24.self_attn.q_proj.lora_B.default.weight, model.layers.24.self_attn.v_proj.lora_A.default.weight, model.layers.24.self_attn.v_proj.lora_B.default.weight, model.layers.25.self_attn.q_proj.lora_A.default.weight, model.layers.25.self_attn.q_proj.lora_B.default.weight, model.layers.25.self_attn.v_proj.lora_A.default.weight, model.layers.25.self_attn.v_proj.lora_B.default.weight, model.layers.26.self_attn.q_proj.lora_A.default.weight, model.layers.26.self_attn.q_proj.lora_B.default.weight, model.layers.26.self_attn.v_proj.lora_A.default.weight, model.layers.26.self_attn.v_proj.lora_B.default.weight, model.layers.27.self_attn.q_proj.lora_A.default.weight, model.layers.27.self_attn.q_proj.lora_B.default.weight, model.layers.27.self_attn.v_proj.lora_A.default.weight, model.layers.27.self_attn.v_proj.lora_B.default.weight, model.layers.28.self_attn.q_proj.lora_A.default.weight, model.layers.28.self_attn.q_proj.lora_B.default.weight, model.layers.28.self_attn.v_proj.lora_A.default.weight, model.layers.28.self_attn.v_proj.lora_B.default.weight, model.layers.29.self_attn.q_proj.lora_A.default.weight, model.layers.29.self_attn.q_proj.lora_B.default.weight, model.layers.29.self_attn.v_proj.lora_A.default.weight, model.layers.29.self_attn.v_proj.lora_B.default.weight, model.layers.30.self_attn.q_proj.lora_A.default.weight, model.layers.30.self_attn.q_proj.lora_B.default.weight, model.layers.30.self_attn.v_proj.lora_A.default.weight, model.layers.30.self_attn.v_proj.lora_B.default.weight, model.layers.31.self_attn.q_proj.lora_A.default.weight, model.layers.31.self_attn.q_proj.lora_B.default.weight, model.layers.31.self_attn.v_proj.lora_A.default.weight, model.layers.31.self_attn.v_proj.lora_B.default.weight, model.layers.32.self_attn.q_proj.lora_A.default.weight, model.layers.32.self_attn.q_proj.lora_B.default.weight, model.layers.32.self_attn.v_proj.lora_A.default.weight, model.layers.32.self_attn.v_proj.lora_B.default.weight, model.layers.33.self_attn.q_proj.lora_A.default.weight, model.layers.33.self_attn.q_proj.lora_B.default.weight, model.layers.33.self_attn.v_proj.lora_A.default.weight, model.layers.33.self_attn.v_proj.lora_B.default.weight, model.layers.34.self_attn.q_proj.lora_A.default.weight, model.layers.34.self_attn.q_proj.lora_B.default.weight, model.layers.34.self_attn.v_proj.lora_A.default.weight, model.layers.34.self_attn.v_proj.lora_B.default.weight, model.layers.35.self_attn.q_proj.lora_A.default.weight, model.layers.35.self_attn.q_proj.lora_B.default.weight, model.layers.35.self_attn.v_proj.lora_A.default.weight, model.layers.35.self_attn.v_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "model_path = \"DrissDo/Qwen2.5-3B-JohnMa\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer_path = \"DrissDo/Qwen2.5-3B-JohnMa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048, padding_idx=151665)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=32, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "ids = []\n",
    "questions = []\n",
    "choices_A = []\n",
    "choices_B = []\n",
    "choices_C = []\n",
    "choices_D = []\n",
    "choices_E = []\n",
    "answers=[]\n",
    "with open('/home/ltnga/ITDSIU21079/VMLU/vmlu_v1.5/valid.jsonl', \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                data = json.loads(line)\n",
    "                ids.append(data[\"id\"])\n",
    "                questions.append(data[\"question\"])\n",
    "                answers.append(data[\"answer\"])\n",
    "                choices = data[\"choices\"]\n",
    "                try:\n",
    "                    choices_A.append(choices[0])\n",
    "                except:\n",
    "                    choices_A.append('')\n",
    "                try:\n",
    "                    choices_B.append(choices[1])\n",
    "                except:\n",
    "                    choices_B.append('')\n",
    "                try:\n",
    "                    choices_C.append(choices[2])\n",
    "                except:\n",
    "                    choices_C.append('')\n",
    "                try:\n",
    "                    choices_D.append(choices[3])\n",
    "                except:\n",
    "                    choices_D.append('')\n",
    "                try:\n",
    "                    choices_E.append(choices[4])\n",
    "                except:\n",
    "                    choices_E.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Những yếu tố nào sau đây có thể dẫn đến thâm hụt cán cân thương mại của một nước:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "        \"id\": ids,\n",
    "        \"prompt\": questions,\n",
    "        \"A\": choices_A,\n",
    "        \"B\": choices_B,\n",
    "        \"C\": choices_C,\n",
    "        \"D\": choices_D,\n",
    "        \"anwser\":answers\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anwser\n",
       "C    202\n",
       "D    191\n",
       "A    181\n",
       "B    170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"anwser\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "preamble = \\\n",
    "        'Chỉ đưa ra chữ cái đứng trước câu trả lời đúng (A, B, C, D) của câu hỏi trắc nghiệm sau: '\n",
    "\n",
    "template = Template('$preamble\\n\\n$prompt\\n\\n $a\\n $b\\n $c\\n $d \\nĐáp án:')\n",
    "\n",
    "def format_input(df, idx):\n",
    "    prompt = df.loc[idx, 'prompt']\n",
    "    a = df.loc[idx, 'A']\n",
    "    b = df.loc[idx, 'B']\n",
    "    c = df.loc[idx, 'C']\n",
    "    d = df.loc[idx, 'D']\n",
    "    # e = df.loc[idx, 'E']\n",
    "\n",
    "    input_text = template.substitute(\n",
    "        preamble=preamble, prompt=prompt, a=a, b=b, c=c, d=d)\n",
    "        \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for running inference:  70.54584527015686\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "start = time.time()\n",
    "for idx in df.index:\n",
    "    \n",
    "    inputs = tokenizer(format_input(df, idx), return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "    outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n",
    "    \n",
    "        # inputs = tokenizer(format_input(df, idx), return_tensors=\"pt\").to(device)\n",
    "        # outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=1)\n",
    "    \n",
    "        # inputs = tokenizer(format_input(df, idx), return_tensors=\"pt\").to(device)\n",
    "        # outputs = model.generate(**inputs, max_new_tokens=1)\n",
    "    answer_decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    last_element = answer_decoded[-1]\n",
    "    answer = last_element.split()[-1]\n",
    "    answers.append(answer)\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print('Time taken for running inference: ', duration)\n",
    "df[\"answer_bot\"]=answer\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phần trăm đúng: 27.15%\n"
     ]
    }
   ],
   "source": [
    "df['is_correct'] = df['anwser'] == df['answer_bot']  # Tạo cột kiểm tra đúng/sai\n",
    "correct_count = df['is_correct'].sum()               # Tổng số câu đúng\n",
    "total_count = len(df)                                # Tổng số câu hỏi\n",
    "\n",
    "# Tính % đúng\n",
    "accuracy = (correct_count / total_count) * 100\n",
    "print(f\"Phần trăm đúng: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training Phần trăm đúng: 22.85%\n",
    "# After training Phần trăm đúng: 27.15%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
