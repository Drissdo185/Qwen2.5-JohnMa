{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, field, asdict\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ltnga/LawVN-Instructction-Gen/src/data/data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from llama_index.core import Document\n",
    "\n",
    "# Load data\n",
    "with open(\"/home/ltnga/LawVN-Instructction-Gen/src/data/data.json\") as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "documents = all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Legal Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LegalReference:\n",
    "    \"\"\"Lưu thông tin về tham chiếu pháp lý.\"\"\"\n",
    "    article: Optional[str] = None\n",
    "    paragraph: Optional[str] = None\n",
    "    point: Optional[str] = None\n",
    "    is_current_article: bool = False\n",
    "    raw_text: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata cho chunk pháp lý.\"\"\"\n",
    "    article_id: str\n",
    "    title: str = \"\"\n",
    "    type: str = \"article\"  # article, article_part, paragraph\n",
    "    paragraphs: List[str] = field(default_factory=list)\n",
    "    points: List[str] = field(default_factory=list)\n",
    "    references: List[LegalReference] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class LegalChunk:\n",
    "    \"\"\"Một đoạn văn bản pháp lý với metadata.\"\"\"\n",
    "    text: str\n",
    "    metadata: ChunkMetadata\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Chuyển đổi thành dict tương thích với llama_index\"\"\"\n",
    "        return {\n",
    "            \"text\": self.text,\n",
    "            \"metadata\": asdict(self.metadata),\n",
    "            \"embedding\": self.embedding.tolist() if self.embedding is not None else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocument:\n",
    "    \"\"\"Xử lý tài liệu pháp lý để chunking và embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_tokens=500, overlap_tokens=100):\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.reference_patterns = {\n",
    "            \"other_article\": re.compile(r'(điểm|khoản) ([a-z]|\\d+)(?:,\\s*(điểm|khoản) ([a-z]|\\d+))* (Điều|khoản) (\\d+)', re.IGNORECASE),\n",
    "            \"same_article\": re.compile(r'(điểm|khoản) ([a-z]|\\d+)(?:,\\s*(điểm|khoản) ([a-z]|\\d+))* (Điều này)', re.IGNORECASE),\n",
    "            \"article_only\": re.compile(r'Điều (\\d+)', re.IGNORECASE)\n",
    "        }\n",
    "    \n",
    "    def process(self, text: str) -> List[LegalChunk]:\n",
    "        \"\"\"Xử lý văn bản pháp lý thành các chunk.\"\"\"\n",
    "        # Phân tách văn bản thành các điều\n",
    "        articles = re.split(r'(Điều \\d+\\.)', text)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(1, len(articles), 2):\n",
    "            article_title = articles[i].strip()\n",
    "            article_content = articles[i+1].strip() if i+1 < len(articles) else \"\"\n",
    "            \n",
    "            # Trích xuất số điều\n",
    "            article_id = re.search(r'Điều (\\d+)\\.', article_title).group(1)\n",
    "            \n",
    "            # Trích xuất tiêu đề điều\n",
    "            title_match = re.search(r'^([^0-9\\.\\n]*)(?=\\d+\\.|$)', article_content)\n",
    "            title = title_match.group(1).strip() if title_match else \"\"\n",
    "            \n",
    "            # Quyết định phương pháp phân chunk\n",
    "            article_chunks = self._process_article(\n",
    "                article_title, article_id, title, article_content\n",
    "            )\n",
    "            chunks.extend(article_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_article(self, article_title, article_id, title, article_content) -> List[LegalChunk]:\n",
    "        \"\"\"Xử lý một điều thành các chunk.\"\"\"\n",
    "        # Tìm các tham chiếu trong điều\n",
    "        references = self._extract_references(article_content, article_id)\n",
    "        \n",
    "        # Đếm tokens (ước tính theo số từ)\n",
    "        token_count = len(article_content.split())\n",
    "        \n",
    "        if token_count <= self.max_chunk_tokens:\n",
    "            # Điều ngắn: giữ nguyên toàn bộ\n",
    "            metadata = ChunkMetadata(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                type=\"article\",\n",
    "                references=references\n",
    "            )\n",
    "            \n",
    "            return [LegalChunk(\n",
    "                text=article_title + \" \" + article_content,\n",
    "                metadata=metadata\n",
    "            )]\n",
    "        else:\n",
    "            # Điều dài: phân theo khoản\n",
    "            return self._split_by_paragraphs(\n",
    "                article_title, article_id, title, article_content, references\n",
    "            )\n",
    "    \n",
    "    def _split_by_paragraphs(\n",
    "        self, article_title, article_id, title, article_content, references\n",
    "    ) -> List[LegalChunk]:\n",
    "        \"\"\"Phân tách điều thành các chunk theo khoản.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Tách khoản\n",
    "        paragraphs = re.split(r'(\\d+\\.)', article_content)\n",
    "        current_chunk_text = article_title + \" \" + title\n",
    "        current_paragraphs = []\n",
    "        current_refs = []\n",
    "        \n",
    "        for j in range(1, len(paragraphs), 2):\n",
    "            if j+1 < len(paragraphs):\n",
    "                para_num = paragraphs[j].strip().replace(\".\", \"\")\n",
    "                para_content = paragraphs[j+1].strip()\n",
    "                \n",
    "                # Tìm tham chiếu trong khoản\n",
    "                para_refs = self._extract_references(para_content, article_id)\n",
    "                \n",
    "                # Kiểm tra độ dài sau khi thêm khoản mới\n",
    "                candidate = current_chunk_text + \" \" + para_num + \". \" + para_content\n",
    "                candidate_tokens = len(candidate.split())\n",
    "                \n",
    "                if candidate_tokens <= self.max_chunk_tokens:\n",
    "                    # Thêm khoản vào chunk hiện tại\n",
    "                    current_chunk_text = candidate\n",
    "                    current_paragraphs.append(para_num)\n",
    "                    current_refs.extend(para_refs)\n",
    "                else:\n",
    "                    # Lưu chunk hiện tại\n",
    "                    metadata = ChunkMetadata(\n",
    "                        article_id=article_id,\n",
    "                        title=title,\n",
    "                        type=\"article_part\",\n",
    "                        paragraphs=current_paragraphs.copy(),\n",
    "                        references=current_refs.copy()\n",
    "                    )\n",
    "                    \n",
    "                    chunks.append(LegalChunk(\n",
    "                        text=current_chunk_text,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "                    \n",
    "                    # Bắt đầu chunk mới với bối cảnh\n",
    "                    # Thêm tiêu đề điều và khoản này\n",
    "                    current_chunk_text = f\"{article_title} (tiếp) {para_num}. {para_content}\"\n",
    "                    current_paragraphs = [para_num]\n",
    "                    current_refs = para_refs.copy()\n",
    "        \n",
    "        # Thêm chunk cuối cùng\n",
    "        if current_paragraphs:\n",
    "            metadata = ChunkMetadata(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                type=\"article_part\",\n",
    "                paragraphs=current_paragraphs,\n",
    "                references=current_refs\n",
    "            )\n",
    "            \n",
    "            chunks.append(LegalChunk(\n",
    "                text=current_chunk_text,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_references(self, text: str, current_article_id: str) -> List[LegalReference]:\n",
    "        \"\"\"Trích xuất tham chiếu từ văn bản.\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        # Tìm tham chiếu đến điều khác\n",
    "        for match in self.reference_patterns[\"other_article\"].finditer(text):\n",
    "            ref = LegalReference(\n",
    "                article=match.group(6),\n",
    "                paragraph=match.group(2) if match.group(1).lower() == \"khoản\" else None,\n",
    "                point=match.group(2) if match.group(1).lower() == \"điểm\" else None,\n",
    "                raw_text=match.group(0)\n",
    "            )\n",
    "            references.append(ref)\n",
    "        \n",
    "        # Tìm tham chiếu trong cùng điều\n",
    "        for match in self.reference_patterns[\"same_article\"].finditer(text):\n",
    "            ref = LegalReference(\n",
    "                article=current_article_id,\n",
    "                paragraph=match.group(2) if match.group(1).lower() == \"khoản\" else None,\n",
    "                point=match.group(2) if match.group(1).lower() == \"điểm\" else None,\n",
    "                is_current_article=True,\n",
    "                raw_text=match.group(0)\n",
    "            )\n",
    "            references.append(ref)\n",
    "        \n",
    "        # Tìm tham chiếu chỉ đến điều\n",
    "        for match in self.reference_patterns[\"article_only\"].finditer(text):\n",
    "            article_id = match.group(1)\n",
    "            # Tránh trùng lặp với các điều đã tìm thấy\n",
    "            if not any(r.article == article_id and r.paragraph is None and r.point is None for r in references):\n",
    "                ref = LegalReference(\n",
    "                    article=article_id,\n",
    "                    raw_text=match.group(0)\n",
    "                )\n",
    "                references.append(ref)\n",
    "        \n",
    "        return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class LegalEmbedding:\n",
    "    \"\"\"Tạo và quản lý embedding cho tài liệu pháp lý.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def create_embeddings(self, chunks: List[LegalChunk], method='enhanced') -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding cho các chunk theo phương pháp chỉ định.\"\"\"\n",
    "        if method == 'basic':\n",
    "            return self._create_basic_embeddings(chunks)\n",
    "        elif method == 'enhanced':\n",
    "            return self._create_enhanced_embeddings(chunks)\n",
    "        elif method == 'hierarchical':\n",
    "            return self._create_hierarchical_embeddings(chunks)\n",
    "        else:\n",
    "            raise ValueError(f\"Phương pháp embedding không hợp lệ: {method}\")\n",
    "    \n",
    "    def _create_basic_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding cơ bản cho các chunk.\"\"\"\n",
    "        texts = [chunk.text for chunk in chunks]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.embedding = embeddings[i]\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_enhanced_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding nâng cao với tính năng pháp lý.\"\"\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # 1. Tạo văn bản nâng cao với trọng số cho các phần quan trọng\n",
    "            enhanced_text = chunk.text\n",
    "            \n",
    "            # Tăng cường tiêu đề điều và số điều\n",
    "            article_title_match = re.search(r'(Điều \\d+\\.\\s*[^\\.\\']+)', enhanced_text)\n",
    "            if article_title_match:\n",
    "                article_title = article_title_match.group(1)\n",
    "                # Lặp lại tiêu đề 2 lần để tăng trọng số trong embedding\n",
    "                enhanced_text = article_title + \" \" + article_title + \" \" + enhanced_text\n",
    "            \n",
    "            # Tăng cường số khoản, điểm\n",
    "            para_points = re.findall(r'(\\d+\\.\\s*[^\\.\\']+|[a-z]\\)\\s*[^;]+)', enhanced_text)\n",
    "            if para_points:\n",
    "                # Thêm các khoản, điểm vào đầu văn bản để tăng trọng số\n",
    "                para_points_text = \" \".join(para_points[:3])  # Giới hạn 3 khoản/điểm đầu tiên\n",
    "                enhanced_text = para_points_text + \" \" + enhanced_text\n",
    "            \n",
    "            # 2. Tạo embedding cho văn bản đã tăng cường\n",
    "            chunk.embedding = self.model.encode(enhanced_text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_hierarchical_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding phân cấp cho các chunk.\"\"\"\n",
    "        \n",
    "        # Tổ chức chunks theo Điều\n",
    "        article_dict = {}\n",
    "        for chunk in chunks:\n",
    "            article_id = chunk.metadata.article_id\n",
    "            if article_id not in article_dict:\n",
    "                article_dict[article_id] = []\n",
    "            article_dict[article_id].append(chunk)\n",
    "        \n",
    "        # Tạo embedding cấp Điều\n",
    "        article_embeddings = {}\n",
    "        for article_id, article_chunks in article_dict.items():\n",
    "            # Ghép tất cả chunk của điều này\n",
    "            full_article_text = \" \".join([chunk.text for chunk in article_chunks])\n",
    "            \n",
    "            # Tạo embedding cấp Điều\n",
    "            article_embeddings[article_id] = self.model.encode(full_article_text)\n",
    "        \n",
    "        # Kết hợp embedding cấp Điều với embedding cấp chunk\n",
    "        for chunk in chunks:\n",
    "            # Tạo embedding cấp chunk\n",
    "            chunk_embedding = self.model.encode(chunk.text)\n",
    "            \n",
    "            # Kết hợp với embedding cấp Điều (với trọng số)\n",
    "            article_embedding = article_embeddings[chunk.metadata.article_id]\n",
    "            combined_embedding = 0.7 * chunk_embedding + 0.3 * article_embedding\n",
    "            \n",
    "            # Chuẩn hóa\n",
    "            norm = np.linalg.norm(combined_embedding)\n",
    "            if norm > 0:\n",
    "                combined_embedding = combined_embedding / norm\n",
    "            \n",
    "            chunk.embedding = combined_embedding\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def search(self, query: str, chunks: List[LegalChunk], top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Tìm kiếm các chunk liên quan đến truy vấn.\"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        results = []\n",
    "        for chunk in chunks:\n",
    "            if chunk.embedding is not None:\n",
    "                # Tính độ tương đồng cosine\n",
    "                similarity = np.dot(query_embedding, chunk.embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk.embedding)\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"chunk\": chunk,\n",
    "                    \"similarity\": float(similarity)\n",
    "                })\n",
    "        \n",
    "        # Sắp xếp theo độ tương đồng\n",
    "        results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        \n",
    "        # Chuyển đổi kết quả sang định dạng dễ sử dụng\n",
    "        formatted_results = []\n",
    "        for result in results[:top_k]:\n",
    "            chunk = result[\"chunk\"]\n",
    "            formatted_results.append({\n",
    "                \"text\": chunk.text,\n",
    "                \"metadata\": asdict(chunk.metadata),\n",
    "                \"similarity\": result[\"similarity\"]\n",
    "            })\n",
    "        \n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Documents Using Custom Legal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents using the custom legal document processor\n",
    "legal_processor = LegalDocument(max_chunk_tokens=500, overlap_tokens=100)\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    # Xử lý mỗi tài liệu\n",
    "    chunks = legal_processor.process(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Đã tạo {len(all_chunks)} chunks từ tài liệu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all chunks\n",
    "legal_embedder = LegalEmbedding(model_name='qducnguyen/vietnamese-bi-encoder')\n",
    "chunks_with_embeddings = legal_embedder.create_embeddings(all_chunks, method='enhanced')\n",
    "\n",
    "print(f\"Đã tạo embedding cho {len(chunks_with_embeddings)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to format compatible with llama_index\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import NodeRelationship, TextNode\n",
    "\n",
    "# Create TextNodes from LegalChunks\n",
    "nodes = []\n",
    "for chunk in chunks_with_embeddings:\n",
    "    # Create a TextNode\n",
    "    node = TextNode(\n",
    "        text=chunk.text,\n",
    "        metadata=asdict(chunk.metadata),\n",
    "        embedding=chunk.embedding\n",
    "    )\n",
    "    nodes.append(node)\n",
    "\n",
    "print(f\"Created {len(nodes)} TextNodes for indexing\")\n",
    "if len(nodes) > 0:\n",
    "    print(f\"Sample node metadata: {nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnamese Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenize text using ViTokenizer\n",
    "for node in tqdm(nodes):\n",
    "    # Save original text in metadata\n",
    "    node.metadata[\"original_text\"] = node.text\n",
    "    # Tokenize and lowercase\n",
    "    node.text = ViTokenizer.tokenize(node.text.lower())\n",
    "    \n",
    "    # Exclude original text from embedding\n",
    "    if not hasattr(node, \"excluded_embed_metadata_keys\"):\n",
    "        node.excluded_embed_metadata_keys = []\n",
    "    if not hasattr(node, \"excluded_llm_metadata_keys\"):\n",
    "        node.excluded_llm_metadata_keys = []\n",
    "    \n",
    "    node.excluded_embed_metadata_keys.append(\"original_text\")\n",
    "    node.excluded_llm_metadata_keys.append(\"original_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "import weaviate\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "\n",
    "WEAVIATE_URL = \"https://jd11sxlqap7tdknwzega.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
    "weaviate_api_key = \"93M51uT7bsG5EMnfL5z78woitWLg7XuAn4ps\"\n",
    "DATA_COLLECTION = \"ND168_LEGAL_ENHANCED\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure vector store for pre-computed embeddings\n",
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client,\n",
    "    index_name=DATA_COLLECTION,\n",
    "    text_key=\"text\",\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create the index using pre-computed embeddings\n",
    "index = VectorStoreIndex(\n",
    "    nodes, \n",
    "    storage_context=storage_context,\n",
    "    embed_model=None,  # Use None since we already have embeddings\n",
    "    insert_batch_size=32768,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "# Setup retriever\n",
    "retriever = index.as_retriever(\n",
    "    vector_store_query_mode=\"hybrid\",\n",
    "    similarity_top_k=10, \n",
    "    alpha=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with our custom search function first\n",
    "TEST_QUESTION = \"đi xe máy không đội mũ bảo hiểm bị phạt bao nhiêu tiền?\"\n",
    "tokenized_query = ViTokenizer.tokenize(TEST_QUESTION.lower())\n",
    "\n",
    "print(\"=== Tìm kiếm với LegalEmbedding.search ===\\n\")\n",
    "custom_results = legal_embedder.search(tokenized_query, chunks_with_embeddings, top_k=5)\n",
    "\n",
    "for i, result in enumerate(custom_results):\n",
    "    print(f\"\\nKết quả #{i+1} (Độ tương đồng: {result['similarity']:.4f})\")\n",
    "    print(f\"Điều {result['metadata']['article_id']}\")\n",
    "    print(f\"Đoạn: {result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with llama_index retriever\n",
    "print(\"=== Tìm kiếm với llama_index retriever ===\\n\")\n",
    "retrievals = retriever.retrieve(tokenized_query)\n",
    "\n",
    "for i, node in enumerate(retrievals[:5]):\n",
    "    print(f\"\\nKết quả #{i+1} (Độ tương đồng: {node.score:.4f})\")\n",
    "    display_source_node(node, source_length=200, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with other approaches (optional)\n",
    "TEST_QUESTIONS = [\n",
    "    \"đi xe máy không đội mũ bảo hiểm bị phạt bao nhiêu tiền?\",\n",
    "    \"phạt bao nhiêu cho người vượt đèn đỏ?\",\n",
    "    \"điều khiển phương tiện mà trong máu có nồng độ cồn bị xử phạt thế nào?\"\n",
    "]\n",
    "\n",
    "for question in TEST_QUESTIONS:\n",
    "    tokenized_query = ViTokenizer.tokenize(question.lower())\n",
    "    print(f\"\\n=== Câu hỏi: {question} ===\\n\")\n",
    "    \n",
    "    # Get results with llama_index retriever\n",
    "    retrievals = retriever.retrieve(tokenized_query)\n",
    "    \n",
    "    # Display top result\n",
    "    if retrievals:\n",
    "        top_node = retrievals[0]\n",
    "        print(f\"Kết quả hàng đầu (Điểm: {top_node.score:.4f})\")\n",
    "        print(f\"Điều: {top_node.metadata.get('article_id', 'N/A')}\")\n",
    "        print(f\"Text: {top_node.text[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
